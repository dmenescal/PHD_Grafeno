{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Writing Custom Datasets, DataLoaders and Transforms\n",
        "**Author**: [Sasank Chilamkurthy](https://chsasank.github.io)\n",
        "\n",
        "A lot of effort in solving any machine learning problem goes into\n",
        "preparing the data. PyTorch provides many tools to make data loading\n",
        "easy and hopefully, to make your code more readable. In this tutorial,\n",
        "we will see how to load and preprocess/augment data from a non trivial\n",
        "dataset.\n",
        "\n",
        "To run this tutorial, please make sure the following packages are\n",
        "installed:\n",
        "\n",
        "-  ``scikit-image``: For image io and transforms\n",
        "-  ``pandas``: For easier csv parsing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.pyplot._IonContext at 0x1db3921b748>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import print_function, division\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "plt.ion()   # interactive mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset we are going to deal with is that of facial pose.\n",
        "This means that a face is annotated like this:\n",
        "\n",
        ".. figure:: /_static/img/landmarked_face2.png\n",
        "   :width: 400\n",
        "\n",
        "Over all, 68 different landmark points are annotated for each face.\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Download the dataset from [here](https://download.pytorch.org/tutorial/faces.zip)\n",
        "    so that the images are in a directory named 'data/faces/'.\n",
        "    This dataset was actually\n",
        "    generated by applying excellent [dlib's pose\n",
        "    estimation](https://blog.dlib.net/2014/08/real-time-face-pose-estimation.html)_\n",
        "    on a few images from imagenet tagged as 'face'.</p></div>\n",
        "\n",
        "Dataset comes with a csv file with annotations which looks like this:\n",
        "\n",
        "::\n",
        "\n",
        "    image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y\n",
        "    0805personali01.jpg,27,83,27,98, ... 84,134\n",
        "    1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312\n",
        "\n",
        "Let's take a single image name and its annotations from the CSV, in this case row index number 65\n",
        "for person-7.jpg just as an example. Read it, store the image name in ``img_name`` and store its\n",
        "annotations in an (L, 2) array ``landmarks`` where L is the number of landmarks in that row.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image name: cone_luz_sem_grafeno_n2_(11.6964_+_14.8i).jpg\n",
            "Deciles shape: (10,)\n",
            "First 4 Deciles: [0.05960265 0.0397351  0.06622517 0.07284768]\n"
          ]
        }
      ],
      "source": [
        "deciles_frame = pd.read_csv('final_image_database_loss_deciles.csv')\n",
        "\n",
        "n = 65\n",
        "img_name = deciles_frame.iloc[n, 0]\n",
        "deciles = deciles_frame.iloc[n, 2:12]\n",
        "deciles = np.asarray(deciles).astype('float')#.reshape(-1, 10)\n",
        "\n",
        "print('Image name: {}'.format(img_name))\n",
        "print('Deciles shape: {}'.format(deciles.shape))\n",
        "print('First 4 Deciles: {}'.format(deciles[:4]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's write a simple helper function to show an image and its landmarks\n",
        "and use it to show a sample.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def show_landmarks(image):\n",
        "    \"\"\"Show image with landmarks\"\"\"\n",
        "    plt.imshow(image)\n",
        "\n",
        "plt.figure()\n",
        "show_landmarks(io.imread(os.path.join('root/imgs/', img_name)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset class\n",
        "\n",
        "``torch.utils.data.Dataset`` is an abstract class representing a\n",
        "dataset.\n",
        "Your custom dataset should inherit ``Dataset`` and override the following\n",
        "methods:\n",
        "\n",
        "-  ``__len__`` so that ``len(dataset)`` returns the size of the dataset.\n",
        "-  ``__getitem__`` to support the indexing such that ``dataset[i]`` can\n",
        "   be used to get $i$\\ th sample.\n",
        "\n",
        "Let's create a dataset class for our face landmarks dataset. We will\n",
        "read the csv in ``__init__`` but leave the reading of images to\n",
        "``__getitem__``. This is memory efficient because all the images are not\n",
        "stored in the memory at once but read as required.\n",
        "\n",
        "Sample of our dataset will be a dict\n",
        "``{'image': image, 'landmarks': landmarks}``. Our dataset will take an\n",
        "optional argument ``transform`` so that any required processing can be\n",
        "applied on the sample. We will see the usefulness of ``transform`` in the\n",
        "next section.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class WaveGuideDataset(Dataset):\n",
        "    \"\"\"Light Waveguide dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.deciles_frame = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.deciles_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                self.deciles_frame.iloc[idx, 0])\n",
        "        image = io.imread(img_name)\n",
        "        deciles = self.deciles_frame.iloc[idx, 2:12]\n",
        "        deciles = np.array([deciles]).astype('float').reshape(-1, 10)\n",
        "        sample = {'image': image, 'deciles': deciles}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's instantiate this class and iterate through the data samples. We\n",
        "will print the sizes of first 4 samples and show their landmarks.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 (442, 442, 3) (1, 10)\n",
            "1 (442, 442, 3) (1, 10)\n",
            "2 (442, 442, 3) (1, 10)\n",
            "3 (442, 442, 3) (1, 10)\n",
            "4 (442, 442, 3) (1, 10)\n",
            "5 (442, 442, 3) (1, 10)\n",
            "6 (442, 442, 3) (1, 10)\n",
            "7 (442, 442, 3) (1, 10)\n",
            "8 (442, 442, 3) (1, 10)\n",
            "9 (442, 442, 3) (1, 10)\n",
            "10 (442, 442, 3) (1, 10)\n",
            "11 (442, 442, 3) (1, 10)\n",
            "12 (442, 442, 3) (1, 10)\n",
            "13 (442, 442, 3) (1, 10)\n",
            "14 (442, 442, 3) (1, 10)\n",
            "15 (442, 442, 3) (1, 10)\n",
            "16 (442, 442, 3) (1, 10)\n",
            "17 (442, 442, 3) (1, 10)\n",
            "18 (442, 442, 3) (1, 10)\n",
            "19 (442, 442, 3) (1, 10)\n",
            "20 (442, 442, 3) (1, 10)\n",
            "21 (442, 442, 3) (1, 10)\n",
            "22 (442, 442, 3) (1, 10)\n",
            "23 (442, 442, 3) (1, 10)\n",
            "24 (442, 442, 3) (1, 10)\n",
            "25 (442, 442, 3) (1, 10)\n",
            "26 (442, 442, 3) (1, 10)\n",
            "27 (442, 442, 3) (1, 10)\n",
            "28 (442, 442, 3) (1, 10)\n",
            "29 (442, 442, 3) (1, 10)\n",
            "30 (442, 442, 3) (1, 10)\n",
            "31 (442, 442, 3) (1, 10)\n",
            "32 (442, 442, 3) (1, 10)\n",
            "33 (442, 442, 3) (1, 10)\n",
            "34 (442, 442, 3) (1, 10)\n",
            "35 (442, 442, 3) (1, 10)\n",
            "36 (442, 442, 3) (1, 10)\n",
            "37 (442, 442, 3) (1, 10)\n",
            "38 (442, 442, 3) (1, 10)\n",
            "39 (442, 442, 3) (1, 10)\n",
            "40 (442, 442, 3) (1, 10)\n",
            "41 (442, 442, 3) (1, 10)\n",
            "42 (442, 442, 3) (1, 10)\n",
            "43 (442, 442, 3) (1, 10)\n",
            "44 (442, 442, 3) (1, 10)\n",
            "45 (442, 442, 3) (1, 10)\n",
            "46 (442, 442, 3) (1, 10)\n",
            "47 (442, 442, 3) (1, 10)\n",
            "48 (442, 442, 3) (1, 10)\n",
            "49 (442, 442, 3) (1, 10)\n",
            "50 (442, 442, 3) (1, 10)\n",
            "51 (442, 442, 3) (1, 10)\n",
            "52 (442, 442, 3) (1, 10)\n",
            "53 (442, 442, 3) (1, 10)\n",
            "54 (442, 442, 3) (1, 10)\n",
            "55 (442, 442, 3) (1, 10)\n",
            "56 (442, 442, 3) (1, 10)\n",
            "57 (442, 442, 3) (1, 10)\n",
            "58 (442, 442, 3) (1, 10)\n",
            "59 (442, 442, 3) (1, 10)\n",
            "60 (442, 442, 3) (1, 10)\n",
            "61 (442, 442, 3) (1, 10)\n",
            "62 (442, 442, 3) (1, 10)\n",
            "63 (442, 442, 3) (1, 10)\n",
            "64 (442, 442, 3) (1, 10)\n",
            "65 (442, 442, 3) (1, 10)\n",
            "66 (442, 442, 3) (1, 10)\n",
            "67 (442, 442, 3) (1, 10)\n",
            "68 (442, 442, 3) (1, 10)\n",
            "69 (442, 442, 3) (1, 10)\n",
            "70 (442, 442, 3) (1, 10)\n",
            "71 (442, 442, 3) (1, 10)\n",
            "72 (442, 442, 3) (1, 10)\n",
            "73 (442, 442, 3) (1, 10)\n",
            "74 (442, 442, 3) (1, 10)\n",
            "75 (442, 442, 3) (1, 10)\n",
            "76 (442, 442, 3) (1, 10)\n",
            "77 (442, 442, 3) (1, 10)\n",
            "78 (442, 442, 3) (1, 10)\n",
            "79 (442, 442, 3) (1, 10)\n",
            "80 (442, 442, 3) (1, 10)\n",
            "81 (442, 442, 3) (1, 10)\n",
            "82 (442, 442, 3) (1, 10)\n",
            "83 (442, 442, 3) (1, 10)\n",
            "84 (442, 442, 3) (1, 10)\n",
            "85 (442, 442, 3) (1, 10)\n",
            "86 (442, 442, 3) (1, 10)\n",
            "87 (442, 442, 3) (1, 10)\n",
            "88 (442, 442, 3) (1, 10)\n",
            "89 (442, 442, 3) (1, 10)\n",
            "90 (442, 442, 3) (1, 10)\n",
            "91 (442, 442, 3) (1, 10)\n",
            "92 (442, 442, 3) (1, 10)\n",
            "93 (442, 442, 3) (1, 10)\n",
            "94 (442, 442, 3) (1, 10)\n",
            "95 (442, 442, 3) (1, 10)\n",
            "96 (442, 442, 3) (1, 10)\n",
            "97 (442, 442, 3) (1, 10)\n",
            "98 (442, 442, 3) (1, 10)\n",
            "99 (442, 442, 3) (1, 10)\n",
            "100 (442, 442, 3) (1, 10)\n",
            "101 (442, 442, 3) (1, 10)\n",
            "102 (442, 442, 3) (1, 10)\n",
            "103 (442, 442, 3) (1, 10)\n",
            "104 (442, 442, 3) (1, 10)\n",
            "105 (442, 442, 3) (1, 10)\n",
            "106 (442, 442, 3) (1, 10)\n",
            "107 (442, 442, 3) (1, 10)\n",
            "108 (442, 442, 3) (1, 10)\n",
            "109 (442, 442, 3) (1, 10)\n",
            "110 (442, 442, 3) (1, 10)\n",
            "111 (442, 442, 3) (1, 10)\n",
            "112 (442, 442, 3) (1, 10)\n",
            "113 (442, 442, 3) (1, 10)\n",
            "114 (442, 442, 3) (1, 10)\n",
            "115 (442, 442, 3) (1, 10)\n",
            "116 (442, 442, 3) (1, 10)\n",
            "117 (442, 442, 3) (1, 10)\n",
            "118 (442, 442, 3) (1, 10)\n",
            "119 (442, 442, 3) (1, 10)\n",
            "120 (442, 442, 3) (1, 10)\n",
            "121 (442, 442, 3) (1, 10)\n",
            "122 (442, 442, 3) (1, 10)\n",
            "123 (442, 442, 3) (1, 10)\n",
            "124 (442, 442, 3) (1, 10)\n",
            "125 (442, 442, 3) (1, 10)\n",
            "126 (442, 442, 3) (1, 10)\n",
            "127 (442, 442, 3) (1, 10)\n",
            "128 (442, 442, 3) (1, 10)\n",
            "129 (442, 442, 3) (1, 10)\n",
            "130 (442, 442, 3) (1, 10)\n",
            "131 (442, 442, 3) (1, 10)\n",
            "132 (442, 442, 3) (1, 10)\n",
            "133 (442, 442, 3) (1, 10)\n",
            "134 (442, 442, 3) (1, 10)\n",
            "135 (442, 442, 3) (1, 10)\n",
            "136 (442, 442, 3) (1, 10)\n",
            "137 (442, 442, 3) (1, 10)\n",
            "138 (442, 442, 3) (1, 10)\n",
            "139 (442, 442, 3) (1, 10)\n",
            "140 (442, 442, 3) (1, 10)\n",
            "141 (442, 442, 3) (1, 10)\n",
            "142 (442, 442, 3) (1, 10)\n",
            "143 (442, 442, 3) (1, 10)\n",
            "144 (442, 442, 3) (1, 10)\n",
            "145 (442, 442, 3) (1, 10)\n",
            "146 (442, 442, 3) (1, 10)\n",
            "147 (442, 442, 3) (1, 10)\n",
            "148 (442, 442, 3) (1, 10)\n",
            "149 (442, 442, 3) (1, 10)\n",
            "150 (442, 442, 3) (1, 10)\n",
            "151 (442, 442, 3) (1, 10)\n",
            "152 (442, 442, 3) (1, 10)\n",
            "153 (442, 442, 3) (1, 10)\n",
            "154 (442, 442, 3) (1, 10)\n",
            "155 (442, 442, 3) (1, 10)\n",
            "156 (442, 442, 3) (1, 10)\n",
            "157 (442, 442, 3) (1, 10)\n",
            "158 (442, 442, 3) (1, 10)\n",
            "159 (442, 442, 3) (1, 10)\n",
            "160 (442, 442, 3) (1, 10)\n",
            "161 (442, 442, 3) (1, 10)\n",
            "162 (442, 442, 3) (1, 10)\n",
            "163 (442, 442, 3) (1, 10)\n",
            "164 (442, 442, 3) (1, 10)\n",
            "165 (442, 442, 3) (1, 10)\n",
            "166 (442, 442, 3) (1, 10)\n",
            "167 (442, 442, 3) (1, 10)\n",
            "168 (442, 442, 3) (1, 10)\n",
            "169 (442, 442, 3) (1, 10)\n",
            "170 (442, 442, 3) (1, 10)\n",
            "171 (442, 442, 3) (1, 10)\n",
            "172 (442, 442, 3) (1, 10)\n",
            "173 (442, 442, 3) (1, 10)\n",
            "174 (442, 442, 3) (1, 10)\n",
            "175 (442, 442, 3) (1, 10)\n",
            "176 (442, 442, 3) (1, 10)\n",
            "177 (442, 442, 3) (1, 10)\n",
            "178 (442, 442, 3) (1, 10)\n",
            "179 (442, 442, 3) (1, 10)\n",
            "180 (442, 442, 3) (1, 10)\n",
            "181 (442, 442, 3) (1, 10)\n",
            "182 (442, 442, 3) (1, 10)\n",
            "183 (442, 442, 3) (1, 10)\n",
            "184 (442, 442, 3) (1, 10)\n",
            "185 (442, 442, 3) (1, 10)\n",
            "186 (442, 442, 3) (1, 10)\n",
            "187 (442, 442, 3) (1, 10)\n",
            "188 (442, 442, 3) (1, 10)\n",
            "189 (442, 442, 3) (1, 10)\n",
            "190 (442, 442, 3) (1, 10)\n",
            "191 (442, 442, 3) (1, 10)\n",
            "192 (442, 442, 3) (1, 10)\n",
            "193 (442, 442, 3) (1, 10)\n",
            "194 (442, 442, 3) (1, 10)\n",
            "195 (442, 442, 3) (1, 10)\n",
            "196 (442, 442, 3) (1, 10)\n",
            "197 (442, 442, 3) (1, 10)\n",
            "198 (442, 442, 3) (1, 10)\n",
            "199 (442, 442, 3) (1, 10)\n",
            "200 (442, 442, 3) (1, 10)\n",
            "201 (442, 442, 3) (1, 10)\n",
            "202 (442, 442, 3) (1, 10)\n",
            "203 (442, 442, 3) (1, 10)\n",
            "204 (442, 442, 3) (1, 10)\n",
            "205 (442, 442, 3) (1, 10)\n",
            "206 (442, 442, 3) (1, 10)\n",
            "207 (442, 442, 3) (1, 10)\n",
            "208 (442, 442, 3) (1, 10)\n",
            "209 (442, 442, 3) (1, 10)\n",
            "210 (442, 442, 3) (1, 10)\n",
            "211 (442, 442, 3) (1, 10)\n",
            "212 (442, 442, 3) (1, 10)\n",
            "213 (442, 442, 3) (1, 10)\n",
            "214 (442, 442, 3) (1, 10)\n",
            "215 (442, 442, 3) (1, 10)\n",
            "216 (442, 442, 3) (1, 10)\n",
            "217 (442, 442, 3) (1, 10)\n",
            "218 (442, 442, 3) (1, 10)\n",
            "219 (442, 442, 3) (1, 10)\n",
            "220 (442, 442, 3) (1, 10)\n",
            "221 (442, 442, 3) (1, 10)\n",
            "222 (442, 442, 3) (1, 10)\n",
            "223 (442, 442, 3) (1, 10)\n",
            "224 (442, 442, 3) (1, 10)\n",
            "225 (442, 442, 3) (1, 10)\n",
            "226 (442, 442, 3) (1, 10)\n",
            "227 (442, 442, 3) (1, 10)\n",
            "228 (442, 442, 3) (1, 10)\n",
            "229 (442, 442, 3) (1, 10)\n",
            "230 (442, 442, 3) (1, 10)\n",
            "231 (442, 442, 3) (1, 10)\n",
            "232 (442, 442, 3) (1, 10)\n"
          ]
        }
      ],
      "source": [
        "wg_dataset = WaveGuideDataset(csv_file='final_image_database_loss_deciles.csv',\n",
        "                                    root_dir='root/imgs/')\n",
        "\n",
        "for i in range(len(wg_dataset)):\n",
        "    sample = wg_dataset[i]\n",
        "\n",
        "    print(i, sample['image'].shape, sample['deciles'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transforms\n",
        "\n",
        "One issue we can see from the above is that the samples are not of the\n",
        "same size. Most neural networks expect the images of a fixed size.\n",
        "Therefore, we will need to write some preprocessing code.\n",
        "Let's create three transforms:\n",
        "\n",
        "-  ``Rescale``: to scale the image\n",
        "-  ``RandomCrop``: to crop from image randomly. This is data\n",
        "   augmentation.\n",
        "-  ``ToTensor``: to convert the numpy images to torch images (we need to\n",
        "   swap axes).\n",
        "\n",
        "We will write them as callable classes instead of simple functions so\n",
        "that parameters of the transform need not be passed everytime it's\n",
        "called. For this, we just need to implement ``__call__`` method and\n",
        "if required, ``__init__`` method. We can then use a transform like this:\n",
        "\n",
        "::\n",
        "\n",
        "    tsfm = Transform(params)\n",
        "    transformed_sample = tsfm(sample)\n",
        "\n",
        "Observe below how these transforms had to be applied both on the image and\n",
        "landmarks.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, deciles = sample['image'], sample['deciles']\n",
        "\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C x H x W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return {'image': torch.from_numpy(image),\n",
        "                'deciles': torch.from_numpy(deciles)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>In the example above, `RandomCrop` uses an external library's random number generator \n",
        "    (in this case, Numpy's `np.random.int`). This can result in unexpected behavior with `DataLoader` \n",
        "    (see https://pytorch.org/docs/stable/notes/faq.html#my-data-loader-workers-return-identical-random-numbers). \n",
        "    In practice, it is safer to stick to PyTorch's random number generator, e.g. by using `torch.randint` instead.</p></div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compose transforms\n",
        "\n",
        "Now, we apply the transforms on a sample.\n",
        "\n",
        "Let's say we want to rescale the shorter side of the image to 256 and\n",
        "then randomly crop a square of size 224 from it. i.e, we want to compose\n",
        "``Rescale`` and ``RandomCrop`` transforms.\n",
        "``torchvision.transforms.Compose`` is a simple callable class which allows us\n",
        "to do this.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scale = Rescale(256)\n",
        "crop = RandomCrop(128)\n",
        "composed = transforms.Compose([Rescale(256),\n",
        "                               RandomCrop(224)])\n",
        "\n",
        "# Apply each of the above transforms on sample.\n",
        "fig = plt.figure()\n",
        "sample = face_dataset[65]\n",
        "for i, tsfrm in enumerate([scale, crop, composed]):\n",
        "    transformed_sample = tsfrm(sample)\n",
        "\n",
        "    ax = plt.subplot(1, 3, i + 1)\n",
        "    plt.tight_layout()\n",
        "    ax.set_title(type(tsfrm).__name__)\n",
        "    show_landmarks(**transformed_sample)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iterating through the dataset\n",
        "\n",
        "Let's put this all together to create a dataset with composed\n",
        "transforms.\n",
        "To summarize, every time this dataset is sampled:\n",
        "\n",
        "-  An image is read from the file on the fly\n",
        "-  Transforms are applied on the read image\n",
        "-  Since one of the transforms is random, data is augmented on\n",
        "   sampling\n",
        "\n",
        "We can iterate over the created dataset with a ``for i in range``\n",
        "loop as before.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transformed_dataset = FaceLandmarksDataset(csv_file='data/faces/face_landmarks.csv',\n",
        "                                           root_dir='data/faces/',\n",
        "                                           transform=transforms.Compose([\n",
        "                                               Rescale(256),\n",
        "                                               RandomCrop(224),\n",
        "                                               ToTensor()\n",
        "                                           ]))\n",
        "\n",
        "for i in range(len(transformed_dataset)):\n",
        "    sample = transformed_dataset[i]\n",
        "\n",
        "    print(i, sample['image'].size(), sample['landmarks'].size())\n",
        "\n",
        "    if i == 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, we are losing a lot of features by using a simple ``for`` loop to\n",
        "iterate over the data. In particular, we are missing out on:\n",
        "\n",
        "-  Batching the data\n",
        "-  Shuffling the data\n",
        "-  Load the data in parallel using ``multiprocessing`` workers.\n",
        "\n",
        "``torch.utils.data.DataLoader`` is an iterator which provides all these\n",
        "features. Parameters used below should be clear. One parameter of\n",
        "interest is ``collate_fn``. You can specify how exactly the samples need\n",
        "to be batched using ``collate_fn``. However, default collate should work\n",
        "fine for most use cases.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "\n",
        "# Helper function to show a batch\n",
        "def show_landmarks_batch(sample_batched):\n",
        "    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n",
        "    images_batch, landmarks_batch = \\\n",
        "            sample_batched['image'], sample_batched['landmarks']\n",
        "    batch_size = len(images_batch)\n",
        "    im_size = images_batch.size(2)\n",
        "    grid_border_size = 2\n",
        "\n",
        "    grid = utils.make_grid(images_batch)\n",
        "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size + (i + 1) * grid_border_size,\n",
        "                    landmarks_batch[i, :, 1].numpy() + grid_border_size,\n",
        "                    s=10, marker='.', c='r')\n",
        "\n",
        "        plt.title('Batch from dataloader')\n",
        "\n",
        "# if you are using Windows, uncomment the next line and indent the for loop.\n",
        "# you might need to go back and change \"num_workers\" to 0. \n",
        "\n",
        "# if __name__ == '__main__':\n",
        "for i_batch, sample_batched in enumerate(dataloader):\n",
        "    print(i_batch, sample_batched['image'].size(),\n",
        "          sample_batched['landmarks'].size())\n",
        "\n",
        "    # observe 4th batch and stop.\n",
        "    if i_batch == 3:\n",
        "        plt.figure()\n",
        "        show_landmarks_batch(sample_batched)\n",
        "        plt.axis('off')\n",
        "        plt.ioff()\n",
        "        plt.show()\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Afterword: torchvision\n",
        "\n",
        "In this tutorial, we have seen how to write and use datasets, transforms\n",
        "and dataloader. ``torchvision`` package provides some common datasets and\n",
        "transforms. You might not even have to write custom classes. One of the\n",
        "more generic datasets available in torchvision is ``ImageFolder``.\n",
        "It assumes that images are organized in the following way: ::\n",
        "\n",
        "    root/ants/xxx.png\n",
        "    root/ants/xxy.jpeg\n",
        "    root/ants/xxz.png\n",
        "    .\n",
        "    .\n",
        "    .\n",
        "    root/bees/123.jpg\n",
        "    root/bees/nsdf3.png\n",
        "    root/bees/asd932_.png\n",
        "\n",
        "where 'ants', 'bees' etc. are class labels. Similarly generic transforms\n",
        "which operate on ``PIL.Image`` like  ``RandomHorizontalFlip``, ``Scale``,\n",
        "are also available. You can use these to write a dataloader like this: ::\n",
        "\n",
        "  import torch\n",
        "  from torchvision import transforms, datasets\n",
        "\n",
        "  data_transform = transforms.Compose([\n",
        "          transforms.RandomSizedCrop(224),\n",
        "          transforms.RandomHorizontalFlip(),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "      ])\n",
        "  hymenoptera_dataset = datasets.ImageFolder(root='hymenoptera_data/train',\n",
        "                                             transform=data_transform)\n",
        "  dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset,\n",
        "                                               batch_size=4, shuffle=True,\n",
        "                                               num_workers=4)\n",
        "\n",
        "For an example with training code, please see\n",
        ":doc:`transfer_learning_tutorial`.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "snn-pso",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "b1e70f11dca2dc5513a0c6b4fc2ed39d02d5fbfdc883f5701900c44d230e79da"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
