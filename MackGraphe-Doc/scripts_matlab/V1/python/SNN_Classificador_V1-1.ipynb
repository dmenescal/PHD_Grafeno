{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from snn_data_utils import WaveGuideDataset, ToTensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Static Wave Guide Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "data_path = 'root/imgs'\n",
    "data_csv = 'final_image_database_loss_deciles.csv'\n",
    "\n",
    "dtype = torch.float32\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ToTensor()])\n",
    "wg_dataset = WaveGuideDataset(root_dir=data_path, csv_file=data_csv, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_train, wg_val = torch.utils.data.random_split(wg_dataset, [173, 60], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(wg_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(wg_val, batch_size=batch_size, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network (A simple Spiked MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs = 442*442*3\n",
    "num_hidden = 1024\n",
    "num_outputs = 10\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 50\n",
    "beta = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "\n",
    "# Load the network onto CUDA if available\n",
    "net = Net().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traininng the SNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass data into the network, sum the spikes over time\n",
    "# and compare the neuron with the highest number of spikes\n",
    "# with the target\n",
    "\n",
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output, _ = net(data.view(batch_size, -1))\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    print(idx)\n",
    "    print() \n",
    "    # As our net has 10 outputs, we can extract any column that idx maps into (each line has equal values)\n",
    "    acc = np.mean(torch.flatten(targets[:, idx][:, :1]).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, train=False)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = next(enumerate(train_loader))\n",
    "data = targets['image'].to(device, dtype=torch.float)\n",
    "targets = targets['deciles'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 10])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_rec, mem_rec = net(data.view(data.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 10]), torch.Size([4, 10]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_rec[0].size(), targets[:,0].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3049, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(mem_rec[0], targets[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4, 4, 4])\n",
      "tensor([0.0662, 0.1258, 0.0397, 0.1722])\n",
      "Test set accuracy for a single minibatch: 10.10%\n"
     ]
    }
   ],
   "source": [
    "print_batch_accuracy(data, targets[:,0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0\n",
      "Train Set Loss: 149.74\n",
      "Test Set Loss: 141.53\n",
      "tensor([4, 4, 4, 4])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-fdff531a0ef4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;31m# Print train/test loss/accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mtrain_printer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0miter_counter\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-090f6b55d5e3>\u001b[0m in \u001b[0;36mtrain_printer\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Train Set Loss: {loss_hist[counter]:.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Test Set Loss: {test_loss_hist[counter]:.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mprint_batch_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mprint_batch_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-090f6b55d5e3>\u001b[0m in \u001b[0;36mprint_batch_accuracy\u001b[1;34m(data, targets, train)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#print(targets[:, idx][:, :1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# As our net has 10 outputs, we can extract any column that idx maps into (each line has equal values)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 4 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    train_batch = enumerate(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data, targets in train_batch:\n",
    "        data = targets['image'].to(device, dtype=dtype) # Must have dtype explicit param!! Avoid trouble\n",
    "        targets = targets['deciles'].to(device, dtype=dtype) # Must have dtype explicit param!! Avoid trouble\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        for step in range(num_steps):\n",
    "            loss_val += loss(mem_rec[step], targets[:,0])\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_data, test_targets = next(enumerate(test_loader))\n",
    "            test_data = test_targets['image'].to(device, dtype=dtype)\n",
    "            test_targets = test_targets['deciles'].to(device, dtype=dtype)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "            for step in range(num_steps):\n",
    "                test_loss += loss(test_mem[step], test_targets[:,0])\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            if counter % 50 == 0:\n",
    "                train_printer()\n",
    "            counter += 1\n",
    "            iter_counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn-pso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1e70f11dca2dc5513a0c6b4fc2ed39d02d5fbfdc883f5701900c44d230e79da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
